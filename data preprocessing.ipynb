{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhyI8QKKZtWl",
        "outputId": "5241a9b0-b1d9-481f-be7f-53d313ed4620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ifYk5ZzgyHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFXF3RJkD3LU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "import sys\n",
        "import os\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "#sys.path.append(\"..\")\n",
        "\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from datetime import datetime\n",
        "import time\n",
        "from io import StringIO\n",
        "\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "#import sys\n",
        "# sys.path.append('/content/drive/MyDrive')\n",
        "\n",
        "# # Step 3: Now you can import the utils module\n",
        "# import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF_O44H6YFEZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "#sys.path.append(\"..\")\n",
        "#import utils\n",
        "import pickle\n",
        "import copy\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import time\n",
        "from io import StringIO\n",
        "from tqdm import tqdm as tqdm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVq7MmQHYNds"
      },
      "outputs": [],
      "source": [
        "D2C_MIN = 2000 #meters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LAT_MIN = 23.565\n",
        "LAT_MAX = 30.10224\n",
        "LON_MIN = -96.554\n",
        "LON_MAX = -82.784"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgdpJckdYPIo"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/15 month new\"\n",
        "\n",
        "# List of CSV files\n",
        "# l_csv_filename = [\n",
        "#     \"train.csv\"\n",
        "#     #,\"valid.csv\"\n",
        "#     #\"Est-aruba_5x5deg_2019240_2019365_.csv\"\n",
        "# ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tl5aekNBYQnW"
      },
      "outputs": [],
      "source": [
        "pkl_filename = \"ct_2019\\\\ct_2017010203_10_20_simulated.pkl\"\n",
        "pkl_filename_train = \"ct_2019\\\\ct_2017010203_10_20_train.pkl\"\n",
        "pkl_filename_valid = \"ct_2019\\\\ct_2017010203_10_20_valid.pkl\"\n",
        "pkl_filename_test = \"ct_2019\\\\ct_2017010203_10_20_test.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkKHBrgBYSSH"
      },
      "outputs": [],
      "source": [
        "LAT_RANGE = LAT_MAX - LAT_MIN\n",
        "LON_RANGE = LON_MAX - LON_MIN\n",
        "SOG_MAX = 30.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbXOXlFfYSOn"
      },
      "outputs": [],
      "source": [
        "EPOCH = datetime(1970, 1, 1)\n",
        "LAT, LON, SOG, COG, HEADING, NAV_STT, TIMESTAMP, MMSI, SHIPTYPE = list(range(9))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l_l_msg = [] # list of AIS messages, each row is a message (list of AIS attributes)\n",
        "n_error = 0\n",
        "#test_csv_filename=[\"valid.csv\"]\n",
        "import pandas as pd\n",
        "for csv_filename in os.listdir(dataset_path):\n",
        "    data_path = os.path.join(dataset_path,csv_filename)\n",
        "\n",
        "    df=pd.read_csv(data_path)\n",
        "    print(f\"reading: {csv_filename}\")\n",
        "    sog = df['SOG']  # Column index 4 is 'SOG'\n",
        "    status = df['Status']  # Column index 11 is 'Status'\n",
        "\n",
        "    # Create a new column for the filled status using numpy's select for vectorized conditions\n",
        "    conditions = [\n",
        "        status.isnull() & (sog == 0.0),\n",
        "        status.isnull() & (0.1 <= sog) & (sog <= 5.0),\n",
        "        status.isnull() & (0.1 < sog) & (sog <= 8.0),\n",
        "        status.isnull() & (5.1 <= sog) & (sog <= 10.0),\n",
        "        status.isnull() & (10.1 <= sog) & (sog <= 15.0),\n",
        "        status.isnull() & (15.1 <= sog) & (sog <= 30.0),\n",
        "        status.isnull() & (sog > 30.0)\n",
        "    ]\n",
        "\n",
        "    choices = [\n",
        "        6,  # Moored (Status 6)\n",
        "        3,  # Not Under Command (Status 3)\n",
        "        8,  # Engaged in Fishing (Status 8)\n",
        "        4,  # Restricted Maneuverability (Status 4)\n",
        "        1,  # Underway Using Engine (Status 1)\n",
        "        1,  # Underway Using Engine (Status 1)\n",
        "        11  # High-Speed Craft (Status 11)\n",
        "    ]\n",
        "\n",
        "    # Default status for other cases\n",
        "    #default_status = 1\n",
        "\n",
        "    # Apply the conditions and choices, fill in the status where it's NaN\n",
        "    df['Status_new'] = np.select(conditions, choices, status)\n",
        "    df['BaseDateTime'] = pd.to_datetime(df['BaseDateTime'])\n",
        "    df['unix'] = df['BaseDateTime'].astype('int64') // 10**9\n",
        "    l_msg = np.column_stack([\n",
        "    df['LAT'].astype(float),           # Latitude as float\n",
        "    df['LON'].astype(float),           # Longitude as float\n",
        "    df['SOG'].astype(float),           # Speed over ground as float\n",
        "    df['COG'].astype(float),           # Course over ground as float\n",
        "    df['Heading'].astype(float),       # Heading as float\n",
        "    df['Status_new'].astype(int),      # Status as integer\n",
        "    df['unix'].astype(int),            # Unix timestamp as integer\n",
        "    df['MMSI'].astype(int),            # MMSI as integer\n",
        "    df['VesselType'].astype(int)       # Vessel type as integer\n",
        "]).tolist()\n",
        "    l_l_msg.extend(l_msg)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBYMPX4QY_FD",
        "outputId": "92e775d8-6c3e-4589-f037-80a43415aa19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading: AIS_172998267124367251_2326-1729982682125.csv\n",
            "reading: AIS_172998267124367251_2580-1729988258904.csv\n",
            "reading: AIS_172998267124367251_3002-1729996853879.csv\n",
            "reading: AIS_172998267124367251_2818-1729993194492.csv\n",
            "reading: AIS_172998267124367251_3165-1729997206940.csv\n",
            "reading: AIS_172998267124367251_3609-1730002225221.csv\n",
            "reading: AIS_172998378920267252_3751-1729983792111.csv\n",
            "reading: AIS_172998378920267252_4013-1729989594335.csv\n",
            "reading: AIS_172998378920267252_7924-1729995325742.csv\n",
            "reading: AIS_172998378920267252_8319-1730000707261.csv\n",
            "reading: AIS_172998378920267252_8629-1730003940025.csv\n",
            "reading: AIS_172998441826067253_10116-1729989566328.csv\n",
            "reading: AIS_172998441826067253_10264-1729994722223.csv\n",
            "reading: AIS_172998441826067253_10559-1729999529421.csv\n",
            "reading: AIS_172998441826067253_11053-1730005832397.csv\n",
            "reading: AIS_172998441826067253_11316-1730009814786.csv\n",
            "reading: AIS_172998441826067253_11882-1730010195940.csv\n",
            "reading: AIS_172998441826067253_9482-1729984423790.csv\n",
            "reading: AIS_172998471230367254_12416-1729984730052.csv\n",
            "reading: AIS_172998471230367254_12516-1729989664886.csv\n",
            "reading: AIS_172998471230367254_13347-1729994121114.csv\n",
            "reading: AIS_172998471230367254_13600-1729999122487.csv\n",
            "reading: AIS_172998471230367254_13757-1730003265658.csv\n",
            "reading: AIS_172998471230367254_13956-1730003903917.csv\n",
            "reading: AIS_172998471230367254_14056-1730004183995.csv\n",
            "reading: AIS_172998471230367254_14198-1730005803176.csv\n",
            "reading: AIS_172998471230367254_14530-1730008018730.csv\n",
            "reading: AIS_172998471230367254_15377-1730011758187.csv\n",
            "reading: AIS_172998498445167255_15510-1729985001760.csv\n",
            "reading: AIS_172998498445167255_15691-1729989125480.csv\n",
            "reading: AIS_172998498445167255_15923-1729993144907.csv\n",
            "reading: AIS_172998498445167255_16769-1729996477507.csv\n",
            "reading: AIS_172998498445167255_17058-1730000046531.csv\n",
            "reading: AIS_172998498445167255_17314-1730004142463.csv\n",
            "reading: AIS_172998498445167255_17513-1730007827467.csv\n",
            "reading: AIS_172998498445167255_17667-1730010732787.csv\n",
            "reading: AIS_172998498445167255_17794-1730014189518.csv\n",
            "reading: AIS_172998498445167255_18144-1730016025040.csv\n",
            "reading: AIS_172998498445167255_17894-1730015856506.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(l_l_msg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leRPf8N2co3y",
        "outputId": "18580640-f01b-4982-9806-a8e1c5a4db3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55944244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l_l_msg[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgkMpnZbdhfK",
        "outputId": "589a306a-ebf2-4475-cb58-6dccee78f02f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[29.30631, -94.49835, 0.1, 104.8, 334.0, 1.0, 1578009717.0, 353775000.0, 70.0]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/train.csv\"\n",
        "df_test=pd.read_csv(data_path)\n",
        "sog = df_test['SOG']  # Column index 4 is 'SOG'\n",
        "status = df_test['Status']  # Column index 11 is 'Status'\n",
        "\n",
        "# Create a new column for the filled status using numpy's select for vectorized conditions\n",
        "conditions = [\n",
        "    status.isnull() & (sog == 0.0),\n",
        "    status.isnull() & (0.1 <= sog) & (sog <= 5.0),\n",
        "    status.isnull() & (0.1 < sog) & (sog <= 8.0),\n",
        "    status.isnull() & (5.1 <= sog) & (sog <= 10.0),\n",
        "    status.isnull() & (10.1 <= sog) & (sog <= 15.0),\n",
        "    status.isnull() & (15.1 <= sog) & (sog <= 30.0),\n",
        "    status.isnull() & (sog > 30.0)\n",
        "]\n",
        "\n",
        "choices = [\n",
        "    6,  # Moored (Status 6)\n",
        "    3,  # Not Under Command (Status 3)\n",
        "    8,  # Engaged in Fishing (Status 8)\n",
        "    4,  # Restricted Maneuverability (Status 4)\n",
        "    1,  # Underway Using Engine (Status 1)\n",
        "    1,  # Underway Using Engine (Status 1)\n",
        "    11  # High-Speed Craft (Status 11)\n",
        "]\n",
        "\n",
        "# Default status for other cases\n",
        "#default_status = 1\n",
        "\n",
        "# Apply the conditions and choices, fill in the status where it's NaN\n",
        "df_test['Status_new'] = np.select(conditions, choices, status)\n",
        "df_test['BaseDateTime'] = pd.to_datetime(df_test['BaseDateTime'])\n",
        "df_test['unix'] = df_test['BaseDateTime'].astype('int64') // 10**9\n",
        "df=df_test\n",
        "test_l_msg = np.column_stack([\n",
        "    df['LAT'].astype(float),           # Latitude as float\n",
        "    df['LON'].astype(float),           # Longitude as float\n",
        "    df['SOG'].astype(float),           # Speed over ground as float\n",
        "    df['COG'].astype(float),           # Course over ground as float\n",
        "    df['Heading'].astype(float),       # Heading as float\n",
        "    df['Status_new'].astype(int),      # Status as integer\n",
        "    df['unix'].astype(int),            # Unix timestamp as integer\n",
        "    df['MMSI'].astype(int),            # MMSI as integer\n",
        "    df['VesselType'].astype(int)       # Vessel type as integer\n",
        "]).tolist()\n",
        "#LAT, LON, SOG, COG, HEADING, NAV_STT, TIMESTAMP, MMSI, SHIPTYPE = list(range(9))\n",
        "# Display the result\n",
        "#print(list_of_lists)\n"
      ],
      "metadata": {
        "id": "_VBVdv1kQf20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLnxZXZNnWLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdde4895-072f-4213-f730-2b90b51560e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15696148\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[29.37895, -85.61354, 7.7, 283.5, 511.0, 0.0, 1561616697.0, 367298000.0, 90.0]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "print(len(test_l_msg))\n",
        "test_l_msg[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m_msg = np.array(l_l_msg)\n",
        "del l_l_msg\n",
        "print(\"Total number of AIS messages: \",m_msg.shape[0])\n",
        "\n",
        "print(\"Lat min: \",np.min(m_msg[:,LAT]), \"Lat max: \",np.max(m_msg[:,LAT]))\n",
        "print(\"Lon min: \",np.min(m_msg[:,LON]), \"Lon max: \",np.max(m_msg[:,LON]))\n",
        "print(\"Ts min: \",np.min(m_msg[:,TIMESTAMP]), \"Ts max: \",np.max(m_msg[:,TIMESTAMP]))\n",
        "\n",
        "# Convert to suitable timestamp format\n",
        "\n",
        "print(\"Time min: \",datetime.utcfromtimestamp(np.min(m_msg[:,TIMESTAMP])).strftime('%Y-%m-%d %H:%M:%SZ'))\n",
        "print(\"Time max: \",datetime.utcfromtimestamp(np.max(m_msg[:,TIMESTAMP])).strftime('%Y-%m-%d %H:%M:%SZ'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvz2FJB2eU4W",
        "outputId": "1115815b-16ed-4c2f-ed07-1e4ace317cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of AIS messages:  55944244\n",
            "Lat min:  23.49571 Lat max:  30.23999\n",
            "Lon min:  -97.00537 Lon max:  -82.68705\n",
            "Ts min:  1578009600.0 Ts max:  1617321597.0\n",
            "Time min:  2020-01-03 00:00:00Z\n",
            "Time max:  2021-04-01 23:59:57Z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKlfrkzwYSJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0601f39c-d209-4f21-95f8-5195b8ef557d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of AIS messages:  55944244\n",
            "Lat min:  23.565 Lat max:  30.10224\n",
            "Lon min:  -96.55398 Lon max:  -82.784\n",
            "Ts min:  1557100800.0 Ts max:  1611100800.0\n",
            "Time min:  2019-05-06 00:00:00Z\n",
            "Time max:  2021-01-20 00:00:00Z\n"
          ]
        }
      ],
      "source": [
        "m_msgv = np.array(test_l_msg)\n",
        "del test_l_msg\n",
        "print(\"Total number of AIS messages: \",m_msg.shape[0])\n",
        "\n",
        "print(\"Lat min: \",np.min(m_msgv[:,LAT]), \"Lat max: \",np.max(m_msgv[:,LAT]))\n",
        "print(\"Lon min: \",np.min(m_msgv[:,LON]), \"Lon max: \",np.max(m_msgv[:,LON]))\n",
        "print(\"Ts min: \",np.min(m_msgv[:,TIMESTAMP]), \"Ts max: \",np.max(m_msgv[:,TIMESTAMP]))\n",
        "\n",
        "# Convert to suitable timestamp format\n",
        "\n",
        "print(\"Time min: \",datetime.utcfromtimestamp(np.min(m_msgv[:,TIMESTAMP])).strftime('%Y-%m-%d %H:%M:%SZ'))\n",
        "print(\"Time max: \",datetime.utcfromtimestamp(np.max(m_msgv[:,TIMESTAMP])).strftime('%Y-%m-%d %H:%M:%SZ'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6q6V3ivkm67"
      },
      "outputs": [],
      "source": [
        "m_msg = m_msg[m_msg[:,LAT]>=LAT_MIN]\n",
        "m_msg = m_msg[m_msg[:,LAT]<=LAT_MAX]\n",
        "m_msg = m_msg[m_msg[:,LON]>=LON_MIN]\n",
        "m_msg = m_msg[m_msg[:,LON]<=LON_MAX]\n",
        "# SOG\n",
        "m_msg = m_msg[m_msg[:,SOG]>=0]\n",
        "m_msg = m_msg[m_msg[:,SOG]<=SOG_MAX]\n",
        "# COG\n",
        "m_msg = m_msg[m_msg[:,SOG]>=0]\n",
        "m_msg = m_msg[m_msg[:,COG]<=360]\n",
        "# D2C\n",
        "#m_msg = m_msg[m_msg[:,D2C]>=D2C_MIN]\n",
        "\n",
        "# TIME\n",
        "m_msg = m_msg[m_msg[:,TIMESTAMP]>=0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m_msgv = m_msgv[m_msgv[:,LAT]>=LAT_MIN]\n",
        "m_msgv = m_msgv[m_msgv[:,LAT]<=LAT_MAX]\n",
        "m_msgv = m_msgv[m_msgv[:,LON]>=LON_MIN]\n",
        "m_msgv = m_msgv[m_msgv[:,LON]<=LON_MAX]\n",
        "# SOG\n",
        "m_msgv = m_msgv[m_msgv[:,SOG]>=0]\n",
        "m_msgv = m_msgv[m_msgv[:,SOG]<=SOG_MAX]\n",
        "# COG\n",
        "m_msgv = m_msgv[m_msgv[:,SOG]>=0]\n",
        "m_msgv = m_msgv[m_msgv[:,COG]<=360]\n",
        "# D2C\n",
        "#m_msgv = m_msgv[m_msgv[:,D2C]>=D2C_MIN]\n",
        "\n",
        "# TIME\n",
        "m_msgv = m_msgv[m_msgv[:,TIMESTAMP]>=0]"
      ],
      "metadata": {
        "id": "0lYQl828e1fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5neXUZF1kmxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db199b8-fc04-4ba8-9e26-1f4801df0cf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 2.93063100e+01, -9.44983500e+01,  1.00000000e-01,  1.04800000e+02,\n",
              "         3.34000000e+02,  1.00000000e+00,  1.57800972e+09,  3.53775000e+08,\n",
              "         7.00000000e+01]),\n",
              " array([ 2.9378950e+01, -8.5613540e+01,  7.7000000e+00,  2.8350000e+02,\n",
              "         5.1100000e+02,  0.0000000e+00,  1.5616167e+09,  3.6729800e+08,\n",
              "         9.0000000e+01]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "m_msg[0],m_msgv[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "6mdHLCA7kmni",
        "outputId": "2f2107e9-76c7-4c7e-ef20-172cb28d37c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total msgs:  54568096\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'm_msgv' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d7d313bcda09>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mm_msg_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm_msgv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mm_msgv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mm_msg_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm_msgv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mm_msgv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of msgs in the training set: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_msg_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'm_msgv' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"Total msgs: \",len(m_msg))\n",
        "m_msg_train=m_msg\n",
        "del m_msg\n",
        "m_msg_valid=m_msgv\n",
        "#del m_msgv\n",
        "m_msg_test=m_msgv\n",
        "del m_msgv\n",
        "print(\"Number of msgs in the training set: \",len(m_msg_train))\n",
        "print(\"Number of msgs in the validation set: \",len(m_msg_valid))\n",
        "print(\"Number of msgs in the test set: \",len(m_msg_test))\n",
        "\n",
        "\n",
        "## MERGING INTO DICT\n",
        "#======================================\n",
        "# Creating AIS tracks from the list of AIS messages.\n",
        "# Each AIS track is formatted by a dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Convert to dicts of vessel's tracks...\")\n",
        "\n",
        "# Training set\n",
        "Vs_train = dict()\n",
        "for v_msg in tqdm(m_msg_train):\n",
        "    mmsi = int(v_msg[MMSI])\n",
        "    if not (mmsi in list(Vs_train.keys())):\n",
        "        Vs_train[mmsi] = np.empty((0,9))\n",
        "    Vs_train[mmsi] = np.concatenate((Vs_train[mmsi], np.expand_dims(v_msg[:9],0)), axis = 0)\n",
        "\n",
        "del m_msg_train\n",
        "# Validation set\n",
        "Vs_valid = dict()\n",
        "for v_msg in tqdm(m_msg_valid):\n",
        "    mmsi = int(v_msg[MMSI])\n",
        "    if not (mmsi in list(Vs_valid.keys())):\n",
        "        Vs_valid[mmsi] = np.empty((0,9))\n",
        "    Vs_valid[mmsi] = np.concatenate((Vs_valid[mmsi], np.expand_dims(v_msg[:9],0)), axis = 0)\n",
        "\n",
        "#del m_msg_valid\n",
        "# Test set\n",
        "Vs_test = dict()\n",
        "for v_msg in tqdm(m_msg_valid):\n",
        "    mmsi = int(v_msg[MMSI])\n",
        "    if not (mmsi in list(Vs_test.keys())):\n",
        "        Vs_test[mmsi] = np.empty((0,9))\n",
        "    Vs_test[mmsi] = np.concatenate((Vs_test[mmsi], np.expand_dims(v_msg[:9],0)), axis = 0)\n",
        "del m_msg_valid\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phwRkSGtCQWJ",
        "outputId": "09c123de-0481-41f6-e2d8-cb2e77e093eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convert to dicts of vessel's tracks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|██████▊   | 36979674/54568096 [2:32:14<1:54:22, 2563.13it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6Jrdyb1kmkE"
      },
      "outputs": [],
      "source": [
        "for key in Vs_train.keys():\n",
        "  print(len(Vs_train[key]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWojolmWCj1v"
      },
      "outputs": [],
      "source": [
        "for filename, filedict in zip([pkl_filename_train,pkl_filename_valid,pkl_filename_test],\n",
        "                              [Vs_train,Vs_valid,Vs_test]\n",
        "                             ):\n",
        "    print(\"Writing to \", os.path.join(dataset_path,filename),\"...\")\n",
        "    with open(os.path.join(dataset_path,filename),\"wb\") as f:\n",
        "        pickle.dump(filedict,f)\n",
        "    print(\"Total number of tracks: \", len(filedict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg0P7bNrC82H"
      },
      "outputs": [],
      "source": [
        "LAT_RANGE = LAT_MAX - LAT_MIN\n",
        "LON_RANGE = LON_MAX - LON_MIN\n",
        "SPEED_MAX = 30.0  # knots\n",
        "DURATION_MAX = 24 #h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngPuMzx8Dy0Y"
      },
      "outputs": [],
      "source": [
        "FIG_W = 960\n",
        "FIG_H = int(960*LAT_RANGE/LON_RANGE) #533 #768\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "dataset_dir = '/content/drive/MyDrive/data2'\n",
        "\n",
        "# Initialize a list to store the loaded data\n",
        "dict_list = []\n",
        "\n",
        "# Loop through each file in the directory\n",
        "for filename in os.listdir(dataset_dir):\n",
        "    if filename.endswith('.pkl'):\n",
        "        # Load each .pkl file and append to dict_list\n",
        "        with open(os.path.join(dataset_dir, filename), \"rb\") as f:\n",
        "            temp = pickle.load(f)\n",
        "            dict_list.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYMA6RppTDlo"
      },
      "outputs": [],
      "source": [
        "len(dict_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqej06YSVacY"
      },
      "outputs": [],
      "source": [
        "pio=0\n",
        "for Vi in dict_list:\n",
        "  for mmsi in list(Vi.keys()):\n",
        "      pio+=1\n",
        "print(pio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4ygCI5DTFWu"
      },
      "outputs": [],
      "source": [
        "print(\" Remove erroneous timestamps and erroneous speeds...\")\n",
        "Vs = dict()\n",
        "for Vi in dict_list:\n",
        "    #print(filename)\n",
        "\n",
        "    for mmsi in list(Vi.keys()):\n",
        "        # Boundary\n",
        "        lat_idx = np.logical_or((Vi[mmsi][:,LAT] > LAT_MAX),\n",
        "                                (Vi[mmsi][:,LAT] < LAT_MIN))\n",
        "        Vi[mmsi] = Vi[mmsi][np.logical_not(lat_idx)]\n",
        "        lon_idx = np.logical_or((Vi[mmsi][:,LON] > LON_MAX),\n",
        "                                (Vi[mmsi][:,LON] < LON_MIN))\n",
        "        Vi[mmsi] = Vi[mmsi][np.logical_not(lon_idx)]\n",
        "#         # Abnormal timestamps\n",
        "#         abnormal_timestamp_idx = np.logical_or((Vi[mmsi][:,TIMESTAMP] > t_max),\n",
        "#                                                (Vi[mmsi][:,TIMESTAMP] < t_min))\n",
        "#         Vi[mmsi] = Vi[mmsi][np.logical_not(abnormal_timestamp_idx)]\n",
        "        # Abnormal speeds\n",
        "        abnormal_speed_idx = Vi[mmsi][:,SOG] > SPEED_MAX\n",
        "        Vi[mmsi] = Vi[mmsi][np.logical_not(abnormal_speed_idx)]\n",
        "        # Deleting empty keys\n",
        "        if len(Vi[mmsi]) == 0:\n",
        "            del Vi[mmsi]\n",
        "            continue\n",
        "        if mmsi not in list(Vs.keys()):\n",
        "            Vs[mmsi] = Vi[mmsi]\n",
        "            del Vi[mmsi]\n",
        "        else:\n",
        "            Vs[mmsi] = np.concatenate((Vs[mmsi],Vi[mmsi]),axis = 0)\n",
        "            del Vi[mmsi]\n",
        "#del dict_list, Vi, abnormal_speed_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CojxbCKJT52X"
      },
      "outputs": [],
      "source": [
        "del dict_list, Vi, abnormal_speed_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvRj62ToU529"
      },
      "outputs": [],
      "source": [
        "print(len(Vs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGG8cbtzU8S4"
      },
      "outputs": [],
      "source": [
        "Vs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFmDC-sZU_ip"
      },
      "outputs": [],
      "source": [
        "## STEP 2: VOYAGES SPLITTING\n",
        "#======================================\n",
        "# Cutting discontiguous voyages into contiguous ones\n",
        "print(\"Cutting discontiguous voyages into contiguous ones...\")\n",
        "count = 0\n",
        "voyages = dict()\n",
        "INTERVAL_MAX = 2*3600 # 2h\n",
        "for mmsi in list(Vs.keys()):\n",
        "    v = Vs[mmsi]\n",
        "    # Intervals between successive messages in a track\n",
        "    intervals = v[1:,TIMESTAMP] - v[:-1,TIMESTAMP]\n",
        "    idx = np.where(intervals > INTERVAL_MAX)[0]\n",
        "    if len(idx) == 0:\n",
        "        voyages[count] = v\n",
        "        count += 1\n",
        "    else:\n",
        "        tmp = np.split(v,idx+1)\n",
        "        for t in tmp:\n",
        "            voyages[count] = t\n",
        "            count += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mi79xIcVwUP"
      },
      "outputs": [],
      "source": [
        "print(len(voyages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwYLN8HMV11X"
      },
      "outputs": [],
      "source": [
        "print(\"Removing AIS track whose length is smaller than 20 or those last less than 4h...\")\n",
        "\n",
        "for k in list(voyages.keys()):\n",
        "    duration = voyages[k][-1,TIMESTAMP] - voyages[k][0,TIMESTAMP]\n",
        "    if (len(voyages[k]) < 20) or (duration < 4*3600):\n",
        "        voyages.pop(k, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpvhQgJhV911"
      },
      "outputs": [],
      "source": [
        "print(len(voyages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uZvIfF437ID"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import os\n",
        "from scipy import interpolate\n",
        "import scipy.ndimage as ndimage\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "import sys\n",
        "# sys.path.append('..')\n",
        "# sys.path.append('Data')\n",
        "#import shapefile\n",
        "import time\n",
        "from pyproj import Geod\n",
        "geod = Geod(ellps='WGS84')\n",
        "#import dataset\n",
        "\n",
        "AVG_EARTH_RADIUS = 6378.137  # in km\n",
        "SPEED_MAX = 30 # knot\n",
        "FIG_DPI = 150\n",
        "\n",
        "#LAT, LON, SOG, COG, HEADING, ROT, NAV_STT, TIMESTAMP, MMSI = list(range(9))\n",
        "\n",
        "def trackOutlier(A):\n",
        "    \"\"\"\n",
        "    Koyak algorithm to perform outlier identification\n",
        "    Our approach to outlier detection is to begin by evaluating the expression\n",
        "    “observation r is anomalous with respect to observation s ” with respect to\n",
        "    every pair of measurements in a track. We address anomaly criteria below;\n",
        "    assume for now that a criterion has been adopted and that the anomaly\n",
        "    relationship is symmetric. More precisely, let a(r,s) = 1 if r and s are\n",
        "    anomalous and a(r,s) = 0 otherwise; symmetry implies that a(r,s) = a(s,r).\n",
        "    If a(r,s) = 1 either one or both of observations are potential outliers,\n",
        "    but which of the two should be treated as such cannot be resolved using\n",
        "    this information alone.\n",
        "    Let A denote the matrix of anomaly indicators a(r, s) and let b denote\n",
        "    the vector of its row sums. Suppose that observation r is an outlier and\n",
        "    that is the only one present in the track. Because we expect it to be\n",
        "    anomalous with respect to many if not all of the other observations b(r)\n",
        "    should be large, while b(s) = 1 for all s ≠ r . Similarly, if there are\n",
        "    multiple outliers the values of b(r) should be large for those observations\n",
        "    and small for the non-outliers.\n",
        "    Source: \"Predicting vessel trajectories from AIS data using R\", Brian L\n",
        "    Young, 2017\n",
        "    INPUT:\n",
        "        A       : (nxn) symmatic matrix of anomaly indicators\n",
        "    OUTPUT:\n",
        "        o       : n-vector outlier indicators\n",
        "\n",
        "    # FOR TEST\n",
        "    A = np.zeros((5,5))\n",
        "    idx = np.array([[0,2],[1,2],[1,3],[0,3],[2,4],[3,4]])\n",
        "    A[idx[:,0], idx[:,1]] = 1\n",
        "    A[idx[:,1], idx[:,0]] = 1    sampling_track = np.empty((0, 9))\n",
        "    for t in range(int(v[0,TIMESTAMP]), int(v[-1,TIMESTAMP]), 300): # 5 min\n",
        "        tmp = utils.interpolate(t,v)\n",
        "        if tmp is not None:\n",
        "            sampling_track = np.vstack([sampling_track, tmp])\n",
        "        else:\n",
        "            sampling_track = None\n",
        "            break\n",
        "    \"\"\"\n",
        "    assert (A.transpose() == A).all(), \"A must be a symatric matrix\"\n",
        "    assert ((A==0) | (A==1)).all(), \"A must be a binary matrix\"\n",
        "    # Initialization\n",
        "    n = A.shape[0]\n",
        "    b = np.sum(A, axis = 1)\n",
        "    o = np.zeros(n)\n",
        "    while(np.max(b) > 0):\n",
        "        r = np.argmax(b)\n",
        "        o[r] = 1\n",
        "        b[r] = 0\n",
        "        for j in range(n):\n",
        "            if (o[j] == 0):\n",
        "                b[j] -= A[r,j]\n",
        "    return o.astype(bool)\n",
        "\n",
        "#===============================================================================\n",
        "#===============================================================================\n",
        "def detectOutlier(track, speed_max = SPEED_MAX):\n",
        "    \"\"\"\n",
        "    removeOutlier() removes anomalus AIS messages from AIS track.\n",
        "    An AIS message is considered as beging anomalous if the speed is\n",
        "    infeasible (> speed_max). There are two types of anomalous messages:\n",
        "        - The reported speed is infeasible\n",
        "        - The calculated speed (distance/time) is infeasible\n",
        "\n",
        "    INPUT:\n",
        "        track       : a (nxd) matrix. Each row is an AIS message. The structure\n",
        "                      must follow: [Timestamp, Lat, Lon, Speed]\n",
        "        speed_max   : knot\n",
        "    OUTPUT:\n",
        "        o           : n-vector outlier indicators\n",
        "    \"\"\"\n",
        "    # Remove anomalous reported speed\n",
        "    o_report = track[:,3] > speed_max # Speed in track is in knot\n",
        "    if o_report.all():\n",
        "        return o_report, None\n",
        "    track = track[np.invert(o_report)]\n",
        "    # Calculate speed base on (lon, lat) and time\n",
        "\n",
        "    N = track.shape[0]\n",
        "    # Anomoly indicator matrix\n",
        "    A = np.zeros(shape = (N,N))\n",
        "\n",
        "    # Anomalous calculated-speed\n",
        "    for i in range(1,5):\n",
        "        # the ith diagonal\n",
        "        _, _, d = geod.inv(track[:N-i,2],track[:N-i,1],\n",
        "                           track[i:,2],track[i:,1])\n",
        "        delta_t = track[i:,0] - track[:N-i,0].astype(np.float64)\n",
        "        cond = np.logical_and(delta_t > 2,d/(delta_t+1e-6) > (speed_max*0.514444))\n",
        "        abnormal_idx = np.nonzero(cond)[0]\n",
        "        A[abnormal_idx, abnormal_idx + i] = 1\n",
        "        A[abnormal_idx + i, abnormal_idx] = 1\n",
        "\n",
        "    o_calcul = trackOutlier(A)\n",
        "\n",
        "    return o_report, o_calcul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np0n7PaX5OF6"
      },
      "outputs": [],
      "source": [
        "def createShapefile(shp_fname, Vs):\n",
        "    \"\"\"\n",
        "    Creating AIS shape files\n",
        "    INPUT:\n",
        "        shp_fname    : name of the shapefile\n",
        "        Vs          : AIS data, each element of the dictionary is an AIS track\n",
        "                      whose structure is:\n",
        "                      [Timestamp, MMSI, Lat, Lon, SOG, COG, Heading, ROT, NAV_STT]\n",
        "    \"\"\"\n",
        "    shp = shapefile.Writer(shapefile.POINT)\n",
        "    shp.field('MMSI', 'N', 10)\n",
        "    shp.field('TIMESTAMP', 'N', 12)\n",
        "    shp.field('DATETIME', 'C', 20)\n",
        "    shp.field('LAT','N',10,5)\n",
        "    shp.field('LON','N',10,5)\n",
        "    shp.field('SOG','N', 10,5)\n",
        "    shp.field('COG', 'N', 10,5)\n",
        "    shp.field('HEADING', 'N', 10,5)\n",
        "    shp.field('ROT', 'N', 5)\n",
        "    shp.field('NAV_STT', 'N', 2)\n",
        "    for mmsi in list(Vs.keys()):\n",
        "        for p in Vs[mmsi]:\n",
        "            shp.point(p[LON],p[LAT])\n",
        "            shp.record(p[MMSI],\n",
        "                       p[TIMESTAMP],\n",
        "                       time.strftime('%H:%M:%S %d-%m-%Y', time.gmtime(p[TIMESTAMP])),\n",
        "                       p[LAT],\n",
        "                       p[LON],\n",
        "                       p[SOG],\n",
        "                       p[COG],\n",
        "                       p[HEADING],\n",
        "                       p[ROT],\n",
        "                       p[NAV_STT])\n",
        "    shp.save(shp_fname)\n",
        "\n",
        "#===============================================================================\n",
        "#===============================================================================\n",
        "def interpolate(t, track):\n",
        "    \"\"\"\n",
        "    Interpolating the AIS message of vessel at a specific \"t\".\n",
        "    INPUT:\n",
        "        - t :\n",
        "        - track     : AIS track, whose structure is\n",
        "                     [LAT, LON, SOG, COG, HEADING, ROT, NAV_STT, TIMESTAMP, MMSI]\n",
        "    OUTPUT:\n",
        "        - [LAT, LON, SOG, COG, HEADING, ROT, NAV_STT, TIMESTAMP, MMSI]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    before_p = np.nonzero(t >= track[:,TIMESTAMP])[0]\n",
        "    after_p = np.nonzero(t < track[:,TIMESTAMP])[0]\n",
        "\n",
        "    if (len(before_p) > 0) and (len(after_p) > 0):\n",
        "        apos = after_p[0]\n",
        "        bpos = before_p[-1]\n",
        "        # Interpolation\n",
        "        dt_full = float(track[apos,TIMESTAMP] - track[bpos,TIMESTAMP])\n",
        "        if (abs(dt_full) > 2*3600):\n",
        "            return None\n",
        "        dt_interp = float(t - track[bpos,TIMESTAMP])\n",
        "        try:\n",
        "            az, _, dist = geod.inv(track[bpos,LON],\n",
        "                                   track[bpos,LAT],\n",
        "                                   track[apos,LON],\n",
        "                                   track[apos,LAT])\n",
        "            dist_interp = dist*(dt_interp/dt_full)\n",
        "            lon_interp, lat_interp, _ = geod.fwd(track[bpos,LON], track[bpos,LAT],\n",
        "                                               az, dist_interp)\n",
        "            speed_interp = (track[apos,SOG] - track[bpos,SOG])*(dt_interp/dt_full) + track[bpos,SOG]\n",
        "            course_interp = (track[apos,COG] - track[bpos,COG] )*(dt_interp/dt_full) + track[bpos,COG]\n",
        "            heading_interp = (track[apos,HEADING] - track[bpos,HEADING])*(dt_interp/dt_full) + track[bpos,HEADING]\n",
        "            #rot_interp = (track[apos,ROT] - track[bpos,ROT])*(dt_interp/dt_full) + track[bpos,ROT]\n",
        "            if dt_interp > (dt_full/2):\n",
        "                nav_interp = track[apos,NAV_STT]\n",
        "            else:\n",
        "                nav_interp = track[bpos,NAV_STT]\n",
        "        except:\n",
        "            return None\n",
        "        return np.array([lat_interp, lon_interp,\n",
        "                         speed_interp, course_interp,\n",
        "                         heading_interp,\n",
        "                         nav_interp,t,\n",
        "                         track[0,MMSI]])\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "#===============================================================================\n",
        "#===============================================================================\n",
        "def remove_gaussian_outlier(v_data,quantile=1.64):\n",
        "    \"\"\"\n",
        "    Remove outliers\n",
        "    INPUT:\n",
        "        v_data      : a 1-D array\n",
        "        quantile    :\n",
        "    OUTPUT:\n",
        "        v_filtered  : filtered array\n",
        "    \"\"\"\n",
        "    d_mean = np.mean(v_data)\n",
        "    d_std = np.std(v_data)\n",
        "    idx_normal = np.where(np.abs(v_data-d_mean)<=quantile*d_std)[0] #90%\n",
        "    return v_data[idx_normal]\n",
        "\n",
        "#===============================================================================\n",
        "#===============================================================================\n",
        "def gaussian_filter_with_nan(U,sigma):\n",
        "    \"\"\"\n",
        "    Apply Gaussian filter when the data contain NaN\n",
        "    INPUT:\n",
        "        U           : a 2-D array (matrix)\n",
        "        sigma       : std for the Gaussian kernel\n",
        "    OUTPUT:\n",
        "        Z           : filtered matrix\n",
        "    \"\"\"\n",
        "    V=U.copy()\n",
        "    V[np.isnan(U)]=0\n",
        "    VV= ndimage.gaussian_filter(V,sigma=sigma)\n",
        "\n",
        "    W=0*U.copy()+1\n",
        "    W[np.isnan(U)]=0\n",
        "    WW= ndimage.gaussian_filter(W,sigma=sigma)\n",
        "    Z=VV/WW\n",
        "    return(Z)\n",
        "\n",
        "#===============================================================================\n",
        "#===============================================================================\n",
        "def show_logprob_map(m_map_logprob_mean, m_map_logprob_std, save_dir,\n",
        "                     logprob_mean_min = -10.0, logprob_std_max = 5.0,\n",
        "                     d_scale = 10, inter_method = \"hanning\",\n",
        "                     fig_w = 960, fig_h = 960,\n",
        "                    ):\n",
        "    \"\"\"\n",
        "    Show the map of the mean and the std of the logprob in each cell.\n",
        "    INPUT:\n",
        "        m_map_logprob_mean   : a 2-D array (matrix)\n",
        "        m_map_logprob_std    : a 2-D array (matrix)\n",
        "        save_dir             : directory to save the images\n",
        "    \"\"\"\n",
        "    # Truncate\n",
        "    m_map_logprob_mean[m_map_logprob_mean<logprob_mean_min] = logprob_mean_min\n",
        "    m_map_logprob_std[m_map_logprob_std>logprob_std_max] = logprob_std_max\n",
        "\n",
        "    # Improve the resolution\n",
        "    n_rows, n_cols = m_map_logprob_mean.shape\n",
        "    m_mean = np.zeros((n_rows*d_scale,n_cols*d_scale))\n",
        "    m_std = np.zeros((n_rows*d_scale,n_cols*d_scale))\n",
        "    for i_row in range(m_map_logprob_mean.shape[0]):\n",
        "        for i_col in range(m_map_logprob_mean.shape[1]):\n",
        "            m_mean[d_scale*i_row:d_scale*(i_row+1),d_scale*i_col:d_scale*(i_col+1)] = m_map_logprob_mean[i_row,i_col]\n",
        "            m_std[d_scale*i_row:d_scale*(i_row+1),d_scale*i_col:d_scale*(i_col+1)] = m_map_logprob_std[i_row,i_col]\n",
        "\n",
        "    # Gaussian filter (with NaN)\n",
        "    m_nan_idx = np.isnan(m_mean)\n",
        "    m_mean = gaussian_filter_with_nan(m_mean, sigma=4.0)\n",
        "    m_mean[m_nan_idx] = np.nan\n",
        "    m_std = gaussian_filter_with_nan(m_std, sigma=4.0)\n",
        "    m_nan_idx = np.isnan(m_std)\n",
        "    m_std[m_nan_idx] = np.nan\n",
        "\n",
        "    plt.figure(figsize=(fig_w/FIG_DPI, fig_h/FIG_DPI), dpi=FIG_DPI)\n",
        "    # plt.subplot(1,2,1)\n",
        "    im = plt.imshow(np.flipud(m_mean),interpolation=inter_method)\n",
        "    ax = plt.gca()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir,\"logprob_mean_map.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(fig_w/FIG_DPI, fig_h/FIG_DPI), dpi=FIG_DPI)\n",
        "    # plt.subplot(1,2,2)\n",
        "    im = plt.imshow(np.flipud(m_std),interpolation=inter_method)\n",
        "    ax = plt.gca()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    plt.colorbar(im, cax=cax)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir,\"logprob_std_map.png\"))\n",
        "    plt.close()\n",
        "\n",
        "#===============================================================================\n",
        "#===============================================================================\n",
        "def plot_abnormal_tracks(Vs_background,l_dict_anomaly,\n",
        "                         filepath,\n",
        "                         lat_min,lat_max,lon_min,lon_max,\n",
        "                         onehot_lat_bins,onehot_lon_bins,\n",
        "                         background_cmap = \"Blues\",\n",
        "                         anomaly_cmap = \"autumn\",\n",
        "                         l_coastline_poly = None,\n",
        "                         fig_w = 960, fig_h = 960,\n",
        "                         fig_dpi = 150,\n",
        "                        ):\n",
        "    plt.figure(figsize=(fig_w/FIG_DPI, fig_h/FIG_DPI), dpi=FIG_DPI)\n",
        "    lat_range = lat_max - lat_min\n",
        "    lon_range = lon_max - lon_min\n",
        "    ## Plot background\n",
        "    cmap = plt.cm.get_cmap(background_cmap)\n",
        "    l_keys = list(Vs_background.keys())\n",
        "    N = len(Vs_background)\n",
        "    for d_i in range(N):\n",
        "        key = l_keys[d_i]\n",
        "        c = cmap(float(d_i)/(N-1))\n",
        "        tmp = Vs_background[key]\n",
        "        v_lat = tmp[:,0]*lat_range + lat_min\n",
        "        v_lon = tmp[:,1]*lon_range + lon_min\n",
        "        plt.plot(v_lon,v_lat,color=c,linewidth=0.8)\n",
        "    plt.xlim([lon_min,lon_max])\n",
        "    plt.ylim([lat_min,lat_max])\n",
        "    plt.xlabel(\"Longitude\")\n",
        "    plt.ylabel(\"Latitude\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    ## Coastlines\n",
        "    try:\n",
        "        for point in l_coastline_poly:\n",
        "            poly = np.array(point)\n",
        "            plt.plot(poly[:,0],poly[:,1],color=\"k\",linewidth=0.8)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    ## Plot abnormal tracks\n",
        "    cmap_anomaly = plt.cm.get_cmap(anomaly_cmap)\n",
        "    N_anomaly = len(l_dict_anomaly)\n",
        "    d_i = 0\n",
        "    for D in l_dict_anomaly:\n",
        "        try:\n",
        "            c = cmap_anomaly(float(d_i)/(N_anomaly-1))\n",
        "        except:\n",
        "            c = 'r'\n",
        "        d_i += 1\n",
        "        tmp = D[\"seq\"]\n",
        "        m_log_weights_np = D[\"log_weights\"]\n",
        "        tmp = tmp[12:]\n",
        "        v_lat = (tmp[:,0]/float(onehot_lat_bins))*lat_range + lat_min\n",
        "        v_lon = ((tmp[:,1]-onehot_lat_bins)/float(onehot_lon_bins))*lon_range + lon_min\n",
        "        plt.plot(v_lon,v_lat,color=c,linewidth=1.2)\n",
        "\n",
        "    plt.savefig(filepath,dpi = fig_dpi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekgG5_SqWGLW"
      },
      "outputs": [],
      "source": [
        "print(\"Removing anomalous message...\")\n",
        "error_count = 0\n",
        "tick = time.time()\n",
        "for k in  tqdm(list(voyages.keys())):\n",
        "    track = voyages[k][:,[TIMESTAMP,LAT,LON,SOG]] # [Timestamp, Lat, Lon, Speed]\n",
        "    try:\n",
        "        o_report, o_calcul = detectOutlier(track, speed_max = 30)\n",
        "        if o_report.all() or o_calcul.all():\n",
        "            voyages.pop(k, None)\n",
        "        else:\n",
        "            voyages[k] = voyages[k][np.invert(o_report)]\n",
        "            voyages[k] = voyages[k][np.invert(o_calcul)]\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing trajectory {k}: {e}\")\n",
        "        voyages.pop(k, None)\n",
        "        error_count += 1\n",
        "tok = time.time()\n",
        "print(\"STEP 4: duration = \",(tok - tick)/60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjVNTT7RWL0x"
      },
      "outputs": [],
      "source": [
        "print(len(voyages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1l_m0ZwWedn"
      },
      "outputs": [],
      "source": [
        "error_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5frVKyPm1O9f"
      },
      "outputs": [],
      "source": [
        "print('Sampling...')\n",
        "Vs = dict()\n",
        "count = 0\n",
        "for k in tqdm(list(voyages.keys())):\n",
        "    v = voyages[k]\n",
        "    sampling_track = np.empty((0, 8))\n",
        "    for t in range(int(v[0,TIMESTAMP]), int(v[-1,TIMESTAMP]), 300): # 5 min\n",
        "        tmp = interpolate(t,v)\n",
        "        if tmp is not None:\n",
        "            sampling_track = np.vstack([sampling_track, tmp])\n",
        "        else:\n",
        "            sampling_track = None\n",
        "            break\n",
        "    if sampling_track is not None:\n",
        "        Vs[count] = sampling_track\n",
        "        count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2Yj0qR78Ga2"
      },
      "outputs": [],
      "source": [
        "count #flaw in interpolation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "705gBrUZ4qTU"
      },
      "outputs": [],
      "source": [
        "print('Re-Splitting...')\n",
        "Data = dict()\n",
        "count = 0\n",
        "for k in tqdm(list(Vs.keys())):\n",
        "    v = Vs[k]\n",
        "    # Split AIS track into small tracks whose duration <= 1 day\n",
        "    idx = np.arange(0, len(v), 12*DURATION_MAX)[1:]\n",
        "    tmp = np.split(v,idx)\n",
        "    for subtrack in tmp:\n",
        "        # only use tracks whose duration >= 4 hours\n",
        "        if len(subtrack) >= 12*4:\n",
        "            Data[count] = subtrack\n",
        "            count += 1\n",
        "print(len(Data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uow7fAJI4vJb"
      },
      "outputs": [],
      "source": [
        "print(\"Removing 'moored' or 'at anchor' voyages...\")\n",
        "for k in  tqdm(list(Data.keys())):\n",
        "    d_L = float(len(Data[k]))\n",
        "\n",
        "    if np.count_nonzero(Data[k][:,NAV_STT] == 1)/d_L > 0.7 \\\n",
        "    or np.count_nonzero(Data[k][:,NAV_STT] == 5)/d_L > 0.7:\n",
        "        Data.pop(k,None)\n",
        "        continue\n",
        "    sog_max = np.max(Data[k][:,SOG])\n",
        "    if sog_max < 1.0:\n",
        "        Data.pop(k,None)\n",
        "print(\"\\n\")\n",
        "print(len(Data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Luxnx-Bc4015"
      },
      "outputs": [],
      "source": [
        "print(\"Removing 'low speed' tracks...\")\n",
        "for k in tqdm(list(Data.keys())):\n",
        "    d_L = float(len(Data[k]))\n",
        "    if np.count_nonzero(Data[k][:,SOG] < 2)/d_L > 0.8:\n",
        "        Data.pop(k,None)\n",
        "print(\"\\n\")\n",
        "print(len(Data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Normalisation...')\n",
        "for k in tqdm(list(Data.keys())):\n",
        "    v = Data[k]\n",
        "    v[:,LAT] = (v[:,LAT] - LAT_MIN)/(LAT_MAX-LAT_MIN)\n",
        "    v[:,LON] = (v[:,LON] - LON_MIN)/(LON_MAX-LON_MIN)\n",
        "    v[:,SOG][v[:,SOG] > SPEED_MAX] = SPEED_MAX\n",
        "    v[:,SOG] = v[:,SOG]/SPEED_MAX\n",
        "    v[:,COG] = v[:,COG]/360.0"
      ],
      "metadata": {
        "id": "n_p3HXSF3AD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_filepath=\"/content/prodata/ct_2019/ct_2019_test.pkl\"\n",
        "print(output_filepath)\n"
      ],
      "metadata": {
        "id": "Njixcl0L3ABW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.path.dirname(output_filepath))\n",
        "\n",
        "os.path.exists(os.path.dirname(output_filepath))\n",
        "\n",
        "\n",
        "# In[27]:\n",
        "\n",
        "\n",
        "if not os.path.exists(os.path.dirname(output_filepath)):\n",
        "    os.makedirs(os.path.dirname(output_filepath))"
      ],
      "metadata": {
        "id": "ctoY1cki2_-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(output_filepath,\"wb\") as f:\n",
        "    pickle.dump(Data,f)\n"
      ],
      "metadata": {
        "id": "rAcNhuaa3lRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Path to the pickle file\n",
        "pkl_file_path = \"/content/prodata/ct_2019/ct_2019_test.pkl\"\n",
        "\n",
        "# Load the .pkl file\n",
        "with open(pkl_file_path, \"rb\") as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "# Print or inspect the data\n",
        "print(\"Data loaded from pickle file:\", data)"
      ],
      "metadata": {
        "id": "zH7QM5lr35kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in data.keys():\n",
        "  print(len(data[key]))"
      ],
      "metadata": {
        "id": "cgg6qV6q4EVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df1=pd.read_csv(\"/content/drive/MyDrive/data2/train.csv\")\n",
        "df1"
      ],
      "metadata": {
        "id": "ZvG6kwQA4RIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "len(np.unique(df1['MMSI']))"
      ],
      "metadata": {
        "id": "xef-X2Yv5y1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MzdmcC7x59aw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}